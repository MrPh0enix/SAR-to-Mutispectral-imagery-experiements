{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c43c56be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform\n",
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "from imageio import imread\n",
    "from PIL import Image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1f2a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio #used to handle geoTiffs\n",
    "from rasterio.plot import reshape_as_image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f47629fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(array):\n",
    "    '''function that normalizes an array to 0-1 range (min max norm)'''\n",
    "    minV, maxV = array.min(), array.max()\n",
    "    return (array - minV) / (maxV - minV)\n",
    "\n",
    "def rgb_geotiff_to_numpy(filepath):\n",
    "    '''function that reads the RGB channels from\n",
    "    a multispectral geoTiff and converst it to a RGB image'''\n",
    "    with rasterio.open(filepath) as src:\n",
    "        \n",
    "        red = normalize(src.read(4))  \n",
    "        green = normalize(src.read(3))  \n",
    "        blue = normalize(src.read(2))\n",
    "        \n",
    "        rgb = np.stack([red, green, blue], axis=0)\n",
    "        rgb = reshape_as_image(rgb)\n",
    "        \n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7e3f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_RGB = glob('D:/EcometricProject/Datasets/ROIs1970_fall_s2/*')\n",
    "        \n",
    "# file = np.random.choice(path_RGB)\n",
    "\n",
    "# plt.imshow(rgb_geotiff_to_numpy(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6c2acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sar_geotiff_to_numpy(filepath):\n",
    "    '''function that takes a SAR geoTiff and outputs a numpy array of the bands'''\n",
    "    with rasterio.open(filepath) as src:\n",
    "        \n",
    "        vv = normalize(src.read(1))\n",
    "        vh = normalize(src.read(2))\n",
    "        \n",
    "        stacked = np.stack([vv, vh], axis=0)\n",
    "        stacked = reshape_as_image(stacked)\n",
    "        \n",
    "    return stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965062ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5cc74db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU config\n",
    "import tensorflow as tf\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbc29bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size=1, is_val = False):\n",
    "       \n",
    "        path_SAR = glob('D:/EcometricProject/Datasets/ROIs/ROIs1970_fall_s1/*')\n",
    "        \n",
    "        batch_images = np.random.choice(path_SAR, size=batch_size)\n",
    "\n",
    "        imgs_A = []\n",
    "        imgs_B = []\n",
    "        for img_path in batch_images:\n",
    "            img_B = sar_geotiff_to_numpy(img_path) #SAR image\n",
    "            img_A = rgb_geotiff_to_numpy(img_path.replace('ROIs1970_fall_s1', 'ROIs1970_fall_s2')) #RGB image \n",
    "\n",
    "\n",
    "            # If training => do random flip , this is a trick to avoid overfitting \n",
    "            if not is_val and np.random.random() < 0.5:\n",
    "                img_A = np.fliplr(img_A)\n",
    "                img_B = np.fliplr(img_B)\n",
    "\n",
    "            imgs_A.append(img_A)\n",
    "            imgs_B.append(img_B)\n",
    "            \n",
    "        #imgs_A = np.array(imgs_A)/127.5 - 1.  #RGB images are already normalized\n",
    "        imgs_A = np.array(imgs_A)\n",
    "        imgs_B = np.array(imgs_B) #still researching normalization of SAR images\n",
    "        \n",
    "        return imgs_A, imgs_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8292c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(batch_size=1, is_val=False):\n",
    "        \n",
    "        path_SAR = glob('D:/EcometricProject/Datasets/ROIs/ROIs1970_fall_s1/*')\n",
    "        \n",
    "        n_batches=batch_size\n",
    "        \n",
    "        for i in range(n_batches-1):\n",
    "            batch = path_SAR[i*batch_size:(i+1)*batch_size]\n",
    "            imgs_A, imgs_B = [], []\n",
    "            for img in batch:\n",
    "                img_B = sar_geotiff_to_numpy(img) #SAR image\n",
    "                img_A = rgb_geotiff_to_numpy(img.replace('ROIs1970_fall_s1', 'ROIs1970_fall_s2')) #RGB image\n",
    "                \n",
    "                # when training => do random flip , this is a trick to avoid overfitting \n",
    "                if not is_val and np.random.random() > 0.5:\n",
    "                        img_A = np.fliplr(img_A)\n",
    "                        img_B = np.fliplr(img_B)\n",
    "                \n",
    "                imgs_A.append(img_A)\n",
    "                imgs_B.append(img_B)\n",
    "                \n",
    "            # normalizing the images \n",
    "            #imgs_A = np.array(imgs_A)/127.5 - 1. # RGB images are already normalized.\n",
    "            imgs_A = np.array(imgs_A)\n",
    "            imgs_B = np.array(imgs_B) #still researching normalization of SAR images\n",
    "\n",
    "            yield imgs_A, imgs_B\n",
    "\n",
    "def imread(path):\n",
    "    #open image and conver it to float32.\n",
    "    return np.array(Image.open(path).convert('RGB')).astype(np.float32)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b77f671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2118688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "        \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "        def conv2d(layer_input, filters, f_size=4, bn=True):\n",
    "            \"\"\"Layers used during downsampling\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(dropout_rate)(u)\n",
    "            u = BatchNormalization(momentum=0.8)(u)\n",
    "            u = Concatenate()([u, skip_input]) #skip connection\n",
    "            return u\n",
    "        \n",
    "        d0 = Input(shape=(256,256,2))\n",
    "\n",
    "        # Downsampling\n",
    "        d1 = conv2d(d0, gf, bn=False)\n",
    "        d2 = conv2d(d1, gf*2)\n",
    "        d3 = conv2d(d2, gf*4)\n",
    "        d4 = conv2d(d3, gf*8)\n",
    "        d5 = conv2d(d4, gf*8)\n",
    "        d6 = conv2d(d5, gf*8)\n",
    "        d7 = conv2d(d6, gf*8)\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(d7, d6, gf*8)\n",
    "        u2 = deconv2d(u1, d5, gf*8)\n",
    "        u3 = deconv2d(u2, d4, gf*8)\n",
    "        u4 = deconv2d(u3, d3, gf*4)\n",
    "        u5 = deconv2d(u4, d2, gf*2)\n",
    "        u6 = deconv2d(u5, d1, gf)\n",
    "\n",
    "        u7 = UpSampling2D(size=2)(u6)\n",
    "        output_img = Conv2D(channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u7)\n",
    "\n",
    "        return Model(d0, output_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbdfbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "        # a small function to make one layer of the discriminator\n",
    "        def d_layer(layer_input, filters, f_size=4, bn=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        img_A = Input(shape=img_shape)\n",
    "        img_B = Input(shape=(256,256,2))\n",
    "\n",
    "        # Concatenate image and conditioning image by channels to produce input\n",
    "        combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n",
    "\n",
    "        d1 = d_layer(combined_imgs, df, bn=False)\n",
    "        d2 = d_layer(d1, df*2)\n",
    "        d3 = d_layer(d2, df*4)\n",
    "        d4 = d_layer(d3, df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d2)\n",
    "\n",
    "        return Model([img_A, img_B], validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36107f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape\n",
    "img_rows = 256\n",
    "img_cols = 256\n",
    "channels = 3\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "\n",
    "\n",
    "# Calculate output shape of D (PatchGAN)\n",
    "patch = int(img_rows / 2**2)\n",
    "disc_patch = (patch, patch, 1)\n",
    "\n",
    "# Number of filters in the first layer of G and D\n",
    "gf = 64\n",
    "df = 64\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator()\n",
    "\n",
    "# Input images and their conditioning images\n",
    "img_A = Input(shape=img_shape)\n",
    "img_B = Input(shape=(256,256,2))\n",
    "\n",
    "# By conditioning on B generate a fake version of A\n",
    "fake_A = generator(img_B)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Discriminators determines validity of translated images / condition pairs\n",
    "valid = discriminator([fake_A, img_B])\n",
    "\n",
    "combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
    "combined.compile(loss=['mse', 'mae'],\n",
    "                              loss_weights=[1, 100],\n",
    "                              optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638cedf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images( dataset_name,epoch, batch_i):\n",
    "        '''function to take 3 images and show the results'''\n",
    "        r, c = 2, 3\n",
    "\n",
    "        imgs_A, imgs_B = load_data(batch_size=3)\n",
    "        fake_A = generator.predict(imgs_B)\n",
    "\n",
    "        gen_imgs = np.concatenate([fake_A, imgs_A])\n",
    "\n",
    "        titles = ['Output', 'Ground Truth']\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt])\n",
    "                axs[i, j].set_title(titles[i])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c7c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( dataset_name,epochs, batch_size=1, show_interval=10):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((batch_size,) + disc_patch)\n",
    "        fake = np.zeros((batch_size,) + disc_patch)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_i, (imgs_A, imgs_B) in enumerate(load_batch(batch_size)):\n",
    "\n",
    "                #  Train Discriminator\n",
    "\n",
    "                # Condition on B and generate a translated version\n",
    "                fake_A = generator.predict(imgs_B)\n",
    "                \n",
    "                # Train the discriminators (original images = real / generated = Fake)\n",
    "                d_loss_real = discriminator.train_on_batch([imgs_A, imgs_B], valid)\n",
    "                d_loss_fake = discriminator.train_on_batch([fake_A, imgs_B], fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                \n",
    "                \n",
    "\n",
    "               \n",
    "                #  Train Generator\n",
    "                g_loss = combined.train_on_batch([imgs_A, imgs_B], [valid, imgs_A])\n",
    "\n",
    "                elapsed_time = datetime.datetime.now() - start_time\n",
    "                \n",
    "            # Plot the progress\n",
    "            if epoch%10==0:\n",
    "                  print (\"[Epoch %d/%d]  [D loss: %f, acc: %3d%%] [G loss: %f] time: %s\" % (epoch, epochs,    \n",
    "                                                                        d_loss[0], 100*d_loss[1],\n",
    "                                                                        g_loss[0],\n",
    "                                                                        elapsed_time))\n",
    "            # If at show interval => show generated image samples\n",
    "            if epoch % show_interval == 0:\n",
    "                    show_images(dataset_name,epoch, batch_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82518f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index=0)):\n",
    "    train(\"cityscapes\",epochs=1000, batch_size=4, show_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146d58e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a16cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp.pkl', 'wb') as file:\n",
    "        pickle.dump(generator, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c44e132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665dcac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp.pkl', 'rb') as file:\n",
    "        generator1 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db237a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images_1( dataset_name,epoch, batch_i, generator1):\n",
    "        \n",
    "        r, c = 3, 3\n",
    "\n",
    "        imgs_A, imgs_B = load_data(batch_size=3)\n",
    "        fake_A = generator1.predict(imgs_B)\n",
    "\n",
    "        gen_imgs = np.concatenate([imgs_B, fake_A, imgs_A])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        titles = ['Input', 'Output', 'Ground Truth']\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt])\n",
    "                axs[i, j].set_title(titles[i])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd2e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_1(1,1,1,generator1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcf1036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
